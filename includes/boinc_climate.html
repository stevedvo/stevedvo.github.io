<header>
	<h3 style="margin-top: 0px;">Climateprediction.net</h3>
</header>
<hr/>
<article>
	<p>Climateprediction.net (CPDN) is a distributed computing project to investigate and reduce uncertainties in climate modelling. It aims to do this by running hundreds of thousands of different models (a large climate ensemble) to gain a better understanding of how models are affected by small changes in the many parameters known to influence the global climate.</p>
	<p>Myles Allen first thought about the need for large Climate ensembles in 1997, but was only introduced to the success of SETI@home in 1999. The first funding proposal in April 1999 was rejected as utterly unrealistic.</p>
	<p>Following a presentation at the World Climate Conference in Hamburg in September 1999 and a commentary in Nature entitled "Do it yourself climate prediction" in October 1999, thousands signed up to this supposedly imminently available program. The Dot-com bubble bursting did not help and the project realised they would have to do most of the programming themselves rather than outsourcing.</p>
	<p>It was launched on 12 September 2003 with a Windows Application and by the end of only the following day the project had already exceeded the capacity of the Earth Simulator to become the world's largest climate modelling facility. The following year a BOINC application was launched which enabled volunteers running Linux & Mac OS X to also participate.</p>
	<p>By November 2005, the number of completed results totalled 45,914 classic models, 3,455 thermohaline models, 85,685 BOINC models and 352 sulphur cycle models. This represented over 6 million model years processed.</p>
	<p>In February 2006, the project moved on to more realistic climate models. A BBC Climate Change Experiment was launched, attracting around 23,000 participants on the first day. The transient climate simulation introduced realistic oceans. This allowed the experiment to investigate changes in the climate response as the climate forcings are changed, rather than an equilibrium response to a significant change like doubling the carbon dioxide level. Therefore, the experiment has now moved on to doing a hindcast of 1920 to 2000 as well as a forecast of 2000 to 2080. This model takes much longer.</p>
	<p>In March 2006, a high resolution model was released as another project, the Seasonal Attribution Project.</p>
	<p>In April 2006, the coupled models were found to have a data input problem. The work was useful for a different purpose than advertised. New models had to be handed out.</p>
	<p>CPDN, which is run primarily by Oxford University in England, has harnessed more computing power and generated more data than any other climate modelling project. It has produced over 100 million model years of data so far. As of December 2010, there are more than 32,000 active participants from 147 countries with a total BOINC credit of more than 14 billion, reporting about 90 teraflops (90 trillion operations per second) of processing power.</p>
	<p>The aim of the Climateprediction.net project is to investigate the uncertainties in various parameterisations that have to be made in state-of-the-art climate models. The model is run thousands of times with slight perturbations to various physics parameters (a 'large ensemble') and the project examines how the model output changes. These parameters are not known exactly, and the variations are within what is subjectively considered to be a plausible range. This will allow the project to improve understanding of how sensitive the models are to small changes and also to things like changes in carbon dioxide and sulphur cycle. In the past, estimates of climate change have had to be made using one or, at best, a very small ensemble (tens rather than thousands) of model runs. By using volunteer's computers, the project is improving understanding of, and confidence in, climate change predictions more than would ever be possible using the supercomputers currently available to scientists.</p>
	<p>The Climateprediction.net experiment helps to "improve methods to quantify uncertainties of climate projections and scenarios, including long-term ensemble simulations using complex models", identified by the Intergovernmental Panel on Climate Change (IPCC) in 2001 as a high priority. Hopefully, the experiment will give decision makers a better scientific basis for addressing one of the biggest potential global problems of the century.</p>
	<p>The crux of the problem is that scientists can run models and see that x% of the models warm y degrees in response to z climate forcings, but how do we know x% is a good representation of the probability of that happening in the real world? The answer is that scientists are uncertain about this and want to improve the level of confidence that can be achieved. Some models will be good and some poor at producing past climate when given past climate forcings and initial conditions (a hindcast). It does make sense to trust the models that do well at recreating the past more than those that do poorly. Therefore, models that do poorly will be downweighted.</p>
	<img src="http://boincstats.com/signature/2/user/2736906/project/sig.png" alt="CPDN Stats"/>
</article>

